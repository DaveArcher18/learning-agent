# LearningAgent: Build Your Own Local RAG Assistant (LangChainÂ âœ•Â QwenÂ âœ•Â Qdrant)

> **Purpose of this file**  
> Act as a full **tutorial / README** so that anyoneâ€”â€‹including futureâ€‘youâ€”â€‹can reproduce, understand, and extend this project.  Each section explains *why* a choice was made **before** showing *how* to implement it.

---

## 0â€¯Â·â€¯Why a Localâ€‘First RAG?

| Reason | Explanation |
|--------|-------------|
| **Privacy** | All embeddings & LLM calls stay on your machine (no data leaves your laptop). |
| **Costâ€‘free** | Every component (Ollama, Qdrant, FastEmbed, Exa free tier) can be used with **\$0** recurring cost. |
| **Reâ€‘producible tutorial** | Readers can clone the repo and follow deterministic steps without paid APIs. |
| **Hackability** | Full control of the vector DB, chunking strategy, and model weights. |

---

## 1â€¯Â·â€¯Toolchain at a Glance

| Layer | Tool | Why we chose it |
|-------|------|----------------|
| LLM | **Qwenâ€‘3Â 4B** via **Ollama** | 32â€¯k token context, Apacheâ€‘2 license, runs on MacÂ M1/M2 CPUs fast enough for dev. |
| Vector DB | **Qdrant (embedded)** | Outstanding similarity search, hybrid BM25Â +Â dense, single Â `pip` install or Docker. |
| Embeddings | **FastEmbedÂ +Â FlagEmbedding** | ONNXâ€‘optimised â‡’ 4â€‘10Ã— faster on CPU vs HuggingFace PyTorch models. |
| Orchestration | LangChain | Quick plumbing for loaders, splitters, retrievers, and agents. |
| Web Search | **Exa Search MCP** | AIâ€‘native search API; free credits; returns cleaned fullâ€‘text â‡’ minimal scraping code. |
| (Opt.) Scraping | Playwright MCP | JSâ€‘heavy pages fallback; keep optional to stay lightweight. |

---

## 2â€¯Â·â€¯Installation Checklist (Stepâ€‘byâ€‘Step)

> **Tip** â€“ copyâ€‘paste the code blocks in order.  Each step is idempotent.

```bash
# 2.1 â€ŠCreate & activate environment
conda create -n learning_agent python=3.10 -y
conda activate learning_agent

# 2.2 â€ŠFast package installs via UV
pip install uv  # First time only
uv pip install \
  langchain langchain-qdrant langchain-exa langchain-ollama \
  qdrant-client fastembed pypdf pymupdf rich python-dotenv

# 2.3 â€ŠInstall & pull Qwenâ€‘4B via Ollama
brew install ollama            # macOS; use installer on Windows/Linux
ollama pull qwen3:4b           # Downloads ~2.6â€¯GB quantized model

# 2.4 â€ŠRun Qdrant (embedded) first time only (creates ./qdrant_data)
qdrant --uri sqlite://./qdrant_data  # or docker runÂ â€¦ if you prefer

# 2.5 â€ŠSet your Exa key (get one at exa.ai â†’ account)
export EXA_API_KEY="skâ€‘yourâ€‘exaâ€‘key"
```

---

## 3â€¯Â·â€¯Project Structure

```
learning-agent/
â”œâ”€ ingest.py          # oneâ€‘off script: ingest local docs â†’ Qdrant
â”œâ”€ learning_agent.py  # interactive CLI loop
â”œâ”€ config.yaml        # userâ€‘editable knobs (top_k, thresholds, etc.)
â”œâ”€ requirements.txt   # locked deps (optional, uv handles versions)
â””â”€ README.md          # generated from this document
```

---

## 4â€¯Â·â€¯Ingestion Pipeline

### 4.1  Load Documents
```python
from pathlib import Path
from langchain.document_loaders import PyPDFLoader, TextLoader

def load_files(path: str):
    path = Path(path)
    docs = []
    for file in path.rglob("*"):
        if file.suffix.lower() == ".pdf":
            docs.extend(PyPDFLoader(str(file)).load())
        elif file.suffix.lower() in {".md", ".txt", ".json"}:
            docs.extend(TextLoader(str(file)).load())
    return docs
```

### 4.2 Â Tokenâ€‘Based Chunking  
> **Why 2000â€¯tokens?** â€“ Qwen supports 32â€¯k tokens, so 2â€¯k strikes a balance: large enough for context, small enough for accurate retrieval. 200â€‘token overlap preserves continuity.

```python
from langchain.text_splitter import TokenTextSplitter
splitter = TokenTextSplitter(chunk_size=2000, chunk_overlap=200, model_name="qwen")
chunks = splitter.split_documents(raw_docs)
```

### 4.3 Â CPUâ€‘Efficient Embeddings with FastEmbed
```python
from fastembed import FlagEmbedding

embedder = FlagEmbedding("BAAI/bge-small-en-v1.5")  # blazing fast CPU encoder
vectors = [embedder.embed(doc.page_content) for doc in chunks]
```

### 4.4 Â Store in Qdrant (& Create Collection if missing)
```python
from qdrant_client import QdrantClient, models as qmodels
client = QdrantClient(path="./qdrant_data")

if "kb" not in [c.name for c in client.get_collections().collections]:
    client.create_collection("kb", qmodels.VectorParams(size=embedder.embedding_size, distance=qmodels.Distance.COSINE))

client.upsert("kb", vectors=vectors, payload=[doc.metadata for doc in chunks])
```

> **Motivation** â€“ Keeping collection name short (`kb`) simplifies CLI flags.

---

## 5â€¯Â·â€¯Query Pipeline

1. **Retrieve from Qdrant (Hybrid mode)**
```python
from langchain_qdrant import QdrantVectorStore, RetrievalMode
vector_store = QdrantVectorStore(client, "kb", embedder, retrieval_mode=RetrievalMode.HYBRID)
retriever = vector_store.as_retriever(search_kwargs={"k": config["top_k"]})
```
2. **Optional Multiâ€‘Query Expansion**
```python
from langchain.retrievers.multi_query import MultiQueryRetriever
multi_retriever = MultiQueryRetriever.from_llm(vector_store, llm=qwen_llm)
```
3. **Conditional Web Fallback (Exa MCP)**  â€“ if `retriever.invoke()` returns empty.
4. **Assemble prompt â†’ ask Qwen**.

---

## 6â€¯Â·â€¯CLIÂ Loop (learning_agent.py)
```
> ingest docs or ask questions interactively

Commands:
  :exit        quit
  :ingest PATH ingest new docs on the fly
  :config      show current RAG params
  :memory off  toggle conversation memory
```

---

## 7â€¯Â·â€¯Configuration File (`config.yaml`)
```yaml
model: qwen3:4b
use_memory: true
embedding_model: fastembed-bge-small-en
top_k: 5
similarity_threshold: 0.5
chunk_size: 2000
chunk_overlap: 200
```
Users edit this instead of code. CLI flags override config.

---

## 8â€¯Â·â€¯Advanced RAG Techniques (Motivations & Howâ€‘Tos)

| Technique | Motivation | 10â€‘second Implementation |
|-----------|------------|---------------------------|
| Hybrid Search | Combine dense + BM25 for better recall | `RetrievalMode.HYBRID` in `QdrantVectorStore` |
| Multiâ€‘Query | Query expansion for higher coverage | `MultiQueryRetriever.from_llm(...)` |
| Metadata Filtering | Narrow results by file, date, tag | Ensure metadata stored â†’ use `SelfQueryRetriever` |

> These three give the highest ROI for accuracy without leaving the local stack.

---

## 9â€¯Â·â€¯NextÂ Steps
- [ ] Write `ingest.py` script (use code in Â§4).
- [ ] Implement CLI loop in `learning_agent.py` (use Â§6 logic).
- [ ] Test endâ€‘toâ€‘end with sample PDFs.
- [ ] Add README badges + GIF demo.

Happy hacking! ðŸš€

