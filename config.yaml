# Learning Agent Configuration
# Primary configuration file - API keys are loaded from .env file
# Edit this file to adjust behavior without changing code

# Core application settings
use_memory: true
use_web_fallback: true  
web_results: 3

# LLM Configuration
llm:
  model: "deepseek/deepseek-prover-v2:free"  # OpenRouter model
  model_provider: "openrouter"  # Primary provider: OpenRouter
  temperature: 0.3
  max_tokens: null
  
  # Provider-specific configurations (API keys loaded from environment)
  providers:
    openrouter:
      base_url: "https://openrouter.ai/api/v1"
      model: "deepseek/deepseek-prover-v2:free"
      timeout: 30

# Document Service Configuration (Universal Document Loading)
documents:
  path: "data/documents"
  chunk_size: 4000  # 4000 token chunks as requested
  chunk_overlap: 500  # 500 token overlap as requested  
  preserve_boundaries: true
  
# Retrieval Configuration (Universal Retrieval for All Queries)
retrieval:
  max_chunks: 5
  enable_fallback: true
  
# RAGFlow Configuration (Advanced RAG Pipeline)
ragflow:
  host: "localhost"
  port: 9380
  docker_service: "ragflow"
  knowledge_base: "mathematical_kb"
  enable_citations: true
  enable_layout_parsing: true
  api:
    base_url: "http://localhost:9380"
    key: ""  # Will be loaded from environment RAGFLOW_API_KEY

# BGE-M3 Configuration (Mathematical Embeddings)
bge_m3:
  model_name: "BAAI/bge-m3"
  device: "cpu"
  max_length: 8192
  batch_size: 4
  dense_dim: 512
  enable_sparse: true
  enable_colbert: true
  cache_embeddings: true

# Mathematical Content Processing
mathematical_content:
  chunk_size: 4000  # Aligned with document chunk size
  chunk_overlap_ratio: 0.125  # 500/4000 = 0.125 (12.5% overlap)
  preserve_latex: true
  theorem_classification: true
  proof_chain_tracking: true
  citation_granularity: "sentence"

# RAG Pipeline Configuration
rag:
  retriever: "ragflow"  # Primary retriever: ragflow or qdrant
  top_k: 50
  final_k: 15
  similarity_threshold: 0.5
  enable_reranking: true
  enable_graphrag: true
  enable_raptor: true

# UI Configuration
ui:
  use_markdown_rendering: true
  enable_latex_processing: true
  use_advanced_latex_rendering: true
  code_highlighting: true
  enable_rich_console: true

# Performance Configuration
performance:
  target_response_time: 10
  enable_batch_processing: true
  memory_optimization: true
  cpu_optimization: true

# Observability Configuration  
observability:
  enable_structured_logging: true
  log_level: "INFO"
  enable_metrics: true
  enable_health_checks: true

# Response formatting
prompt_template: |
  Answer the question based on the following context with automatic citations.
  Preserve mathematical notation and provide granular source attribution.

  Context:
  {context}

  Question: {question}

# Legacy settings (for backward compatibility)
legacy_qdrant:
  collection: "MoravaKTheory"
  db_search_limit: 20
